# kodosumi

kodosumi is a runtime environment for managing and executing agentic services at scale. It is built on top of Ray, a distributed computing framework, and combines Litestar and FastAPI to deliver agentic services.

<Note>
This content is automatically synchronized from the [GitHub repository](https://github.com/masumi-network/kodosumi/blob/main/README.md).
</Note>

## Key Components

1. Ray Cluster for executing services
2. Distributed compute infrastructure
3. User's application/service

## Installation and Setup

### Prerequisites
- Ray version 2.46.0
- Python 3.12.6
- OpenAI API key (for some examples)

### Installation Steps

1. Clone kodosumi-examples repository
```bash
git clone https://github.com/masumi-network/kodosumi-examples.git
cd ./kodosumi-examples
git checkout dev
pip install .
```

2. Set OpenAI API key in `.env` file
```
OPENAI_API_KEY=<-- enter-your-key -->
```

3. Start Ray head node
```bash
dotenv run -- ray start --head
```

### Deployment Options

#### Option 1: Run with Uvicorn
```bash
uvicorn agentic.examples.hymn.app:app --port 8011
koco start --register http://localhost:8011/openapi.json
```

#### Option 2: Deploy with Ray Serve
```bash
serve run agentic.examples.hymn.app:fast_app
serve deploy agentic.examples.hymn.app:fast_app
```

### Multi-Service Configuration

Create configuration files in `./data/config/`:
- `config.yaml`: Global configuration
- `hymn.yaml`: Hymn service configuration
- `prime.yaml`: Prime number service configuration
- `throughput.yaml`: Throughput service configuration

## Example Services

- **Hymn**: Creates hymns using CrewAI and OpenAI
- **Prime**: Calculates prime number gaps
- **Throughput**: Demonstrates event stream pressures
- **Form**: Shows supported form elements